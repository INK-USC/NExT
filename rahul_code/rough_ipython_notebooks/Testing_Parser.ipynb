{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import CCG.utils\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Selection = ['NER']\n",
    "\n",
    "\n",
    "def count_sublist(lis,sublis):\n",
    "    cnt = 0\n",
    "    if len(sublis)>len(lis):\n",
    "        return cnt\n",
    "    len_sub = len(sublis)\n",
    "    for st in range(len(lis)-len(sublis)+1):\n",
    "        if lis[st:st+len_sub]==sublis:\n",
    "            cnt+=1\n",
    "    return cnt\n",
    "\n",
    "\n",
    "#function for $And\n",
    "def merge(x,y):\n",
    "    if type(x)!= tuple:\n",
    "        x = [x]\n",
    "    if type(y)!= tuple:\n",
    "        y = [y]\n",
    "    x = list(x)\n",
    "    y = list(y)\n",
    "    return tuple(x+y)\n",
    "\n",
    "#function for $Is\n",
    "def IsFunc(ws,ps,c):\n",
    "    if isinstance(ps,tuple):\n",
    "        bool_list  = []\n",
    "        for p in ps:\n",
    "            if isinstance(ws, tuple):\n",
    "                if ws[0] in Selection:\n",
    "                    bool_list.append(p(ws)(c))\n",
    "                else:\n",
    "                    bool_list.append(all([p(w)(c) for w in ws]))\n",
    "            else:\n",
    "                bool_list.append(p(ws)(c))\n",
    "        return all(bool_list)\n",
    "\n",
    "    if isinstance(ws,tuple):\n",
    "        if ws[0] in Selection:\n",
    "            return ps(ws)(c)\n",
    "        else:\n",
    "            return all([ps(w)(c) for w in ws])\n",
    "    else:\n",
    "        return ps(ws)(c)\n",
    "\n",
    "#function for @Left and @Right\n",
    "def at_POSI(POSI,ws,arg,c,option=None):\n",
    "    if isinstance(ws,tuple):\n",
    "        if ws[0] in Selection:\n",
    "            return ws[1] in c.get_other_posi(POSI,arg[-1])[ws[0]]\n",
    "        else:\n",
    "            bool_list = []\n",
    "            for w in ws:\n",
    "                bool_list.append(at_POSI_0(POSI,arg,w,c,option))\n",
    "            return all(bool_list)\n",
    "    else:\n",
    "        return at_POSI_0(POSI,arg,ws,c,option)\n",
    "\n",
    "#function for @Left0 and @Right0\n",
    "def at_POSI_0(POSI,arg,w,c,option=None):\n",
    "    if arg not in ['ArgX','ArgY']:\n",
    "        w,arg = arg,w\n",
    "        if POSI == 'Left':\n",
    "            POSI = 'Right'\n",
    "        elif POSI == 'Right':\n",
    "            POSI = 'Left'\n",
    "\n",
    "    if isinstance(w,tuple) and w[0] not in Selection:\n",
    "        return all([at_POSI_0(POSI,arg,ww,c,option) for ww in w])\n",
    "\n",
    "    if w=='ArgY':\n",
    "        w = c.obj\n",
    "    elif w=='ArgX':\n",
    "        w = c.subj\n",
    "\n",
    "    if option==None:\n",
    "        option = {'attr': 'word', 'range': -1, 'numAppear':1,'cmp':'nlt','onlyCount':False}  #For now, if onlyCount==True, then attr=='tokens'\n",
    "    if isinstance(w,tuple):\n",
    "        return w[1] in c.get_other_posi(POSI, arg[-1])[w[0]]\n",
    "    else:                                                                                   #For now,option['attr']==tokens if and only if 'right before' is used or onlyCount==True, otherwise ==word\n",
    "        w = w.lower()\n",
    "        w_split = w.split()\n",
    "        while '' in w_split:\n",
    "            w_split.remove('')\n",
    "        if option == 'Direct':\n",
    "            range_ = len(w_split)\n",
    "            info = [token.lower() for token in c.get_other_posi(POSI, arg[-1])['tokens']]\n",
    "            if POSI == 'Left':\n",
    "                st = max(0, len(info) - range_)\n",
    "                info = info[st:]\n",
    "            elif POSI == 'Right':\n",
    "                ed = min(len(info) - 1, range_-1)\n",
    "                info = info[:ed + 1]\n",
    "            else:\n",
    "                raise ValueError\n",
    "            # print(info)\n",
    "            if info==w_split:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        if option['range']==-1:\n",
    "            info = [token.lower() for token in c.get_other_posi(POSI, arg[-1])['tokens']]\n",
    "        else:                                                                               #For now, if range!=-1 then attr == 'tokens'\n",
    "            info = [token.lower() for token in c.get_other_posi(POSI, arg[-1])[option['attr']]]\n",
    "            range_ = option['range']\n",
    "            if POSI == 'Left':\n",
    "                st = max(0, len(info) - range_)\n",
    "                info = info[st:]\n",
    "            elif POSI=='Right':\n",
    "                ed = min(len(info) - 1, range_-1)\n",
    "                info = info[:ed + 1]\n",
    "            else:\n",
    "                count_posi = c.get_other_posi(POSI, arg[-1])['POSI']\n",
    "                st = max(0,count_posi-range_)\n",
    "                ed = min(len(info),count_posi+1+range_)\n",
    "                info = info[st:ed+1]\n",
    "        if option['onlyCount']:\n",
    "            return compare[option['cmp']](len(info),option['numAppear'])\n",
    "        else:\n",
    "            return compare[option['cmp']](count_sublist(info,w_split),option['numAppear'])\n",
    "\n",
    "\n",
    "#function for @Between\n",
    "\n",
    "def at_between(w,c,option=None,a=None):\n",
    "    if w=='ArgY':\n",
    "        w = c.obj\n",
    "    elif w=='ArgX':\n",
    "        w = c.subj\n",
    "    if option==None:\n",
    "        option = {'attr': 'word', 'numAppear':1,'cmp':'nlt','onlyCount':False}                #For now, if onlyCount==True, then attr=='tokens'\n",
    "    if isinstance(w,tuple):\n",
    "        return w[1] in c.get_mid()[w[0]]\n",
    "    else:                                                                                   #For now,option['attr']==tokens if and only if  onlyCount==True, otherwise ==word\n",
    "        w = w.lower()\n",
    "        w_split = w.split()\n",
    "        while '' in w_split:\n",
    "            w_split.remove('')\n",
    "        info = [token.lower() for token in c.get_mid()['tokens']]\n",
    "        if option['onlyCount']:\n",
    "            return compare[option['cmp']](len(info),option['numAppear'])\n",
    "        else:\n",
    "            # print(info, w,info.count(w),type(info.count(w)),option['numAppear'],option['eq'],compare[option['eq']](info.count(w),option['numAppear']))\n",
    "            return compare[option['cmp']](count_sublist(info,w_split),option['numAppear'])\n",
    "\n",
    "\n",
    "#function for counting\n",
    "def at_lessthan(funcx,nouny,w,c):\n",
    "    if w=='There':\n",
    "        onlyCount=True\n",
    "    else:\n",
    "        onlyCount = False\n",
    "    if onlyCount:\n",
    "        return funcx(w,{'attr':nouny['attr'],'range':-1,'numAppear':nouny['num'],'cmp':'lt','onlyCount':onlyCount})(c)                #There are less than 3 words before OBJ\n",
    "    else:\n",
    "        return funcx(w, {'attr': nouny['attr'], 'range': nouny['num'], 'numAppear': 1, 'cmp': 'nlt','onlyCount': onlyCount})(c)      #the word 'x' is less than 3 words before OBJ\n",
    "\n",
    "def at_atmost(funcx,nouny,w,c):\n",
    "    if w=='There':\n",
    "        onlyCount=True\n",
    "    else:\n",
    "        onlyCount = False\n",
    "    if onlyCount:\n",
    "        return funcx(w,{'attr':nouny['attr'],'range':-1,'numAppear':nouny['num'],'cmp':'nmt','onlyCount':onlyCount})(c)                #There are at most 3 words before OBJ\n",
    "    else:\n",
    "        return funcx(w, {'attr': nouny['attr'], 'range': nouny['num'], 'numAppear': 1, 'cmp': 'nlt','onlyCount': onlyCount})(c)      #the word 'x' is at most 3 words before OBJ\n",
    "\n",
    "def at_atleast(funcx,nouny,w,c):\n",
    "    if w=='There':\n",
    "        onlyCount=True\n",
    "    else:\n",
    "        onlyCount = False\n",
    "    if onlyCount:\n",
    "        return funcx(w,{'attr':nouny['attr'],'range':-1,'numAppear':nouny['num'],'cmp':'nlt','onlyCount':onlyCount})(c)             #There are at least 3 words before OBJ\n",
    "    else:\n",
    "        return funcx(w,{'attr': nouny['attr'], 'range': nouny['num'], 'numAppear': 1, 'cmp': 'nlt','onlyCount': onlyCount})(c)      #the word 'x' is no less than 3 words before OBJ\n",
    "\n",
    "def at_morethan(funcx,nouny,w,c):\n",
    "    if w=='There':\n",
    "        onlyCount=True\n",
    "    else:\n",
    "        onlyCount = False\n",
    "    if onlyCount:\n",
    "        return funcx(w,{'attr':nouny['attr'],'range':-1,'numAppear':nouny['num'],'cmp':'mt','onlyCount':onlyCount})(c)                #There are more than 3 words before OBJ\n",
    "    else:\n",
    "        return funcx(w, {'attr': nouny['attr'], 'range': nouny['num'], 'numAppear': 1, 'cmp': 'nlt','onlyCount': onlyCount})(c)    #the word 'x' is more than 3 words before OBJ\n",
    "\n",
    "\n",
    "#function for @In0\n",
    "\n",
    "def at_In0(arg,w,c):\n",
    "    assert arg=='Sentence'\n",
    "    if isinstance(w,tuple):\n",
    "        return w in c.ner\n",
    "    else:\n",
    "        w = w.lower().split()\n",
    "        info = [token.lower() for token in c.token]\n",
    "        return count_sublist(info,w)>0\n",
    "\n",
    "def at_WordCount(nounNum,nouny,F,c):\n",
    "    if isinstance(nouny,tuple):\n",
    "        return all([F(noun, option={'attr': 'word', 'range': -1, 'numAppear': 1, 'cmp': 'nlt', 'onlyCount': False})(c) for noun in nouny]) and F(nouny[0],option={'attr': 'tokens','range': -1,'numAppear':sum([len(noun.split()) for noun in nouny]),'cmp': 'eq','onlyCount': True})(c)\n",
    "    else:\n",
    "        return F(nouny,option={'attr':'word','range':-1,'numAppear':1,'cmp':'nlt','onlyCount':False})(c) and F(nouny,option={'attr':'tokens','range':-1,'numAppear':len(nouny.split()),'cmp':'eq','onlyCount':True})(c)\n",
    "\n",
    "ops={\n",
    "    \".root\":lambda xs:lambda c:all([x(c) for x in xs]) if type(xs)==tuple else xs(c),\n",
    "    \"@Word\":lambda x:x,\n",
    "    \"@Is\":lambda ws,p: lambda c: IsFunc(ws,p,c),\n",
    "    \"@between\": lambda a: lambda w,option=None:lambda c:at_between(w,c,option,a),\n",
    "    \"@And\": lambda x,y:merge(x,y),\n",
    "    \"@Num\": lambda x,y:{'attr':y,'num':int(x)},\n",
    "    \"@LessThan\":lambda funcx,nouny:lambda w:lambda c:at_lessthan(funcx,nouny,w,c),\n",
    "    \"@AtMost\":lambda funcx,nouny:lambda w:lambda c:at_atmost(funcx,nouny,w,c),\n",
    "    \"@AtLeast\":lambda funcx,nouny:lambda w:lambda c:at_atleast(funcx,nouny,w,c),\n",
    "    \"@MoreThan\":lambda funcx,nouny:lambda w:lambda c:at_morethan(funcx,nouny,w,c),\n",
    "    \"@WordCount\":lambda nounNum,nouny,F:lambda useless:lambda c:at_WordCount(nounNum,nouny,F,c),\n",
    "\n",
    "    \"@NumberOf\":lambda x,f:[x,f],\n",
    "    \"@LessThan1\":lambda nounynum:lambda x:lambda c:at_lessthan(x[1],{'attr':x[0],\"num\":int(nounynum)},'There',c),\n",
    "    \"@AtMost1\":lambda nounynum:lambda x:lambda c:at_atmost(x[1],{'attr':x[0],\"num\":int(nounynum)},'There',c),\n",
    "    \"@AtLeast1\":lambda nounynum:lambda x:lambda c:at_atleast(x[1],{'attr':x[0],\"num\":int(nounynum)},'There',c),\n",
    "    \"@MoreThan1\":lambda nounynum:lambda x:lambda c:at_morethan(x[1],{'attr':x[0],\"num\":int(nounynum)},'There',c),\n",
    "\n",
    "\n",
    "    \"@In0\":lambda arg: lambda w:lambda c:at_In0(arg,w,c),\n",
    "    #By\n",
    "    \"@By\":lambda x,f,z:lambda c: f(x,{'attr': z['attr'], 'range': z['num'], 'numAppear': 1, 'cmp': 'nlt','onlyCount': False})(c),\n",
    "    # is xx Arg\n",
    "    \"@Left0\":lambda arg: lambda w,option=None:lambda c:at_POSI_0('Left',arg,w,c,option),\n",
    "    \"@Right0\":lambda arg: lambda w,option=None:lambda c:at_POSI_0('Right',arg,w,c,option),\n",
    "    \"@Range0\":lambda arg: lambda w,option=None:lambda c:at_POSI_0('Range',arg,w,c,option),\n",
    "\n",
    "    \"@Left\":lambda arg, ws,option=None:lambda c:at_POSI('Left',ws,arg,c,option),\n",
    "    \"@Right\":lambda arg, ws,option=None:lambda c:at_POSI('Right',ws,arg,c,option),\n",
    "\n",
    "    \"@Direct\":lambda func:lambda w:lambda c: func(w,'Direct')(c),\n",
    "\n",
    "    \"@StartsWith\": lambda x,y: lambda c: c.with_(x[-1],'starts',y),\n",
    "    \"@EndsWith\": lambda x,y: lambda c:c.with_(x[-1],'ends',y),\n",
    "}\n",
    "\n",
    "\n",
    "syntax = {\n",
    "    \"@LOC\": ('NER','LOCATION'),\n",
    "    \"@Date\":('NER','DATE'),\n",
    "    \"@Num\":('NER','NUMBER'),\n",
    "    \"@Org\":('NER','ORGANIZATION'),\n",
    "    \"@Norp\":('NER','Norp'),\n",
    "    '@PER':('NER','PERSON')\n",
    "}\n",
    "\n",
    "lexicon_head = ''':- S,NP,N,PP\n",
    "        VP :: S\\\\NP\n",
    "        Det :: NP/N\n",
    "        Adj :: N/N'''\n",
    "\n",
    "raw_lexicon =''':- S,NP,N,PP\n",
    "        VP :: S\\\\NP\n",
    "        Det :: NP/N\n",
    "        Adj :: N/N\n",
    "        arg => NP {None}\n",
    "        #$True => (S\\\\VP)/PP {None}\n",
    "        #$False => (S\\\\VP)/PP {None}\n",
    "        $And => var\\\\.,var/.,var {\\\\x y.'@And'(x,y)}\n",
    "        $Or => var\\\\.,var/.,var {\\\\x y.'@Or'(x,y)}\n",
    "        $Not => (S\\\\NP)\\\\(S\\\\NP) {None}\n",
    "        $Not => (S\\\\NP)/(S\\\\NP) {None}\n",
    "        $All => NP/N {None}\n",
    "        $All => NP {None}\n",
    "        $All => NP/NP {None}\n",
    "        $Any => NP/N {None}\n",
    "        $None => N {None}\n",
    "        #$Is => (S\\\\NP)/NP {\\\\y x.'@Is'(x,y)}\n",
    "        #$Is => (S\\\\NP)/(S\\\\NP) {\\\\y x.'@Is'(x,y)}\n",
    "        $Is => (S\\\\NP)/PP {\\\\y x.'@Is'(x,y)}   # word 'a' occurs between <S> and <O>\n",
    "        $Is => (S\\\\NP)\\\\PP {\\\\y x.'@Is'(x,y)}  # between <S> and <O> occurs word 'a'\n",
    "        $Is => (S\\\\PP)\\\\NP {\\\\x y.'@Is'(x,y)}  # between <S> and <O> word 'a' occurs\n",
    "        $Exists => S\\\\NP/PP {\\\\y x.'@Is'(x,y)}\n",
    "        #$Exists => S\\\\NP {None}\n",
    "        $Int => Adj {None} #There are no words between <S> and <O>\n",
    "        $AtLeastOne => NP/N {None}\n",
    "        #$Equals => (S\\\\NP)/NP {None}\n",
    "        #$NotEquals => (S\\\\NP)/NP {None}\n",
    "        \n",
    "        \n",
    "        $LessThan => PP/PP/N {\\\\x y.'@LessThan'(y,x)} #There are less than 3 words between <S> and <O>   \n",
    "        $AtMost => PP/PP/N {\\\\x y.'@AtMost'(y,x)} #There are at most 3 words between <S> and <O>\n",
    "        $AtLeast => PP/PP/N {\\\\x y.'@AtLeast'(y,x)} #same as above\n",
    "        $MoreThan => PP/PP/N {\\\\x y.'@MoreThan'(y,x)} #same as above\n",
    "        \n",
    "        $LessThan => PP/N {\\\\x.'@LessThan1'(y,x)} #number of words between X and Y is less than 7.\n",
    "        $AtMost => PP/N {\\\\x.'@AtMost1'(y,x)} \n",
    "        $AtLeast => PP/N {\\\\x.'@AtLeast1'(y,x)}   #same as above\n",
    "        $MoreThan => PP/N {\\\\x.'@MoreThan1'(y,x)} #same as above\n",
    "\n",
    "        #$In => S\\\\NP/NP {None} \n",
    "        $In => PP/NP {\\\\x.'@In0'(x)} \n",
    "        $Contains => S\\\\NP/NP {None} #The sentence contains two words\n",
    "        $Separator => var\\\\.,var/.,var {\\\\x y.'@And'(x,y)} #connection between two words\n",
    "        #$Processive => NP/N\\\\N {None}\n",
    "        #$Count => N {None}\n",
    "        #$Tuple => N {None}\n",
    "        #$ArgXListAnd => NP {None}\n",
    "        $EachOther => N {None}\n",
    "        $Token => N {\\\\x.'@Word'(x)}\n",
    "        $Word => NP/N {\\\\x.'@Word'(x)}\n",
    "        $Word => NP/NP {\\\\x.'@Word'(x)}\n",
    "        \n",
    "        $Word => N {'tokens'} #There are no more than 3 words between <S> and <O>\n",
    "        $Word => NP {'tokens'} #There are no more than 3 words between <S> and <O>\n",
    "        \n",
    "        $Char => N {None} #same as above\n",
    "        #$Lower => Adj {None}\n",
    "        #$Capital => Adj {None}\n",
    "        $StartsWith => S\\\\NP/NP {\\\\y x.'@StartsWith'(x,y)}\n",
    "        $EndsWith => S\\\\NP/NP {\\\\y x.'@EndsWith'(x,y)}\n",
    "        $Left => PP/NP {\\\\x.'@Left0'(x)} # the word 'a' is before <S>\n",
    "        $Left => (S\\\\NP)/NP {\\\\y x.'@Left'(y,x)}  #Precedes\n",
    "        $Right => PP/NP {\\\\x.'@Right0'(x)}# the word 'a' ia after <S>\n",
    "        $Right => (S\\\\NP)/NP {\\\\y x.'@Right'(y,x)} \n",
    "        #$Within => ((S\\\\NP)\\\\(S\\\\NP))/NP {None} # the word 'a' is within 2 words after <S>\n",
    "        #$Within => (NP\\\\NP)/NP {None}\n",
    "        $Within => PP/PP/N {\\\\x y.'@AtMost'(y,x)} #Does Within has other meaning.\n",
    "        $Sentence => NP {'Sentence'}\n",
    "        \n",
    "        $Between => (S/S)/NP {\\\\x y.'@between'(x,y)}\n",
    "        $Between => S/NP {\\\\x.'@between'(x)}\n",
    "        $Between => PP/NP {\\\\x.'@between'(x)}\n",
    "        $Between => (NP\\\\NP)/NP {\\\\x y.'@between'(x,y)}\n",
    "        \n",
    "        $PersonNER => NP {'@PER'}\n",
    "        $LocationNER => NP {'@LOC'}\n",
    "        $DateNER => NP {'@Date'}\n",
    "        $NumberNER => NP {'@Num'}\n",
    "        $OrganizationNER => NP {'@Org'}\n",
    "        $NorpNER => NP {'@Norp'}\n",
    "        $ArgX => NP {'ArgX'}\n",
    "        $ArgY => NP {'ArgY'}\n",
    "        #$will => S\\\\NP/VP {None}\n",
    "        #$Which => (NP\\\\NP)/(S/NP) {None}\n",
    "        #$might => S\\\\NP/VP {None}\n",
    "        $that => NP/N {None}\n",
    "        #$that => (N\\\\N)/(S/NP) {None} #same as which\n",
    "        $Apart => (S/PP)\\\\NP {None}\n",
    "        $Direct => PP/PP {\\\\x.'@Direct'(x)} # the word 'a' is right before <S>   \n",
    "        $Direct => (S\\\\NP)/PP {\\\\y x.'@Is'(x,'@Direct'(y))}\n",
    "        $Last => Adj {None}\n",
    "        $There => NP {'There'}\n",
    "        $By => S\\\\NP\\\\PP/NP {\\\\z f x.'@By'(x,f,z)} #precedes sth by 10 chatacters       \n",
    "        $By => (S\\\\NP)\\\\PP/(PP/PP) {\\\\F x y.'@Is'(y,F(x))} #precedes sth by no more than10 chatacters        \n",
    "        $By => PP\\\\PP/(PP/PP) {\\\\F x. F(x)} #occurs before by no...\n",
    "        \n",
    "        $Numberof => NP/PP/NP {\\\\x F.'@NumberOf'(x,F)}\n",
    "        \n",
    "        $Of => PP/NP {\\\\x.'@Range0'(x)} # the word 'x' is at most 3 words of Y     \n",
    "        $Of => NP/NP {\\\\x.x} #these two are designed to solve problems like $Is $Left $Of and $Is $Left\n",
    "        $Of => N/N {\\\\x.x}\n",
    "        $Char => NP/N {None}\n",
    "        $ArgX => N {'ArgX'}\n",
    "        $ArgY => N {'ArgY'}\n",
    "        $Link => (S\\\\NP)/NP {\\\\x y.'@Is'(y,'@between'(x))}\n",
    "        $SandWich => (S\\\\NP)/NP {\\\\x y.'@Is'(x,'@between'(y))}\n",
    "        $The => N/N {\\\\x.x}\n",
    "        $The =>NP/NP {\\\\x.x}\n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatch_count_score = 0.5\n",
    "unmatch_count_dist = 7\n",
    "unmatch_match_score = 0.5\n",
    "unmatch_match_dist = 7\n",
    "\n",
    "\n",
    "Selection = ['NER']\n",
    "compare_soft={                   #eq mt(more than) lt(less than) nmt(no more than) nlt(no less than)\n",
    "    'eq':lambda a,b:tf.math.maximum(tf.cast(tf.equal(a,b),tf.float32),tf.cast(tf.logical_and(tf.greater_equal(a,b-unmatch_count_dist),tf.less_equal(a,b+unmatch_count_dist)),tf.float32)*unmatch_count_score),\n",
    "    'mt':lambda a,b:tf.math.maximum(tf.cast(tf.greater(a,b),tf.float32),tf.cast(tf.greater(a,b-unmatch_count_dist),tf.float32)*unmatch_count_score),\n",
    "    'lt':lambda a,b:tf.math.maximum(tf.cast(tf.less(a,b),tf.float32),tf.cast(tf.less(a,b+unmatch_count_dist),tf.float32)*unmatch_count_score),\n",
    "    'nmt':lambda a,b:tf.math.maximum(tf.cast(tf.less_equal(a,b),tf.float32),tf.cast(tf.less_equal(a,b+unmatch_count_dist),tf.float32)*unmatch_count_score),\n",
    "    'nlt':lambda a,b:tf.math.maximum(tf.cast(tf.greater_equal(a,b),tf.float32),tf.cast(tf.greater_equal(a,b-unmatch_count_dist),tf.float32)*unmatch_count_score),\n",
    "}\n",
    "\n",
    "#c: Tensor [seqlen seqlen 2 2] : sentence,ner,subj_posi,obj_posi,subj,obj\n",
    "def get_mid(attr,mask_mat,c):\n",
    "    subj_posi = c[:,-4:-3]\n",
    "    obj_posi = c[:,-3:-2]\n",
    "    seqlen = (c.get_shape()[1]-4)//2\n",
    "    tokens = c[:,0:seqlen]\n",
    "    ner = c[:,seqlen:seqlen*2]\n",
    "    st_posi = tf.math.minimum(subj_posi,obj_posi)\n",
    "    ed_posi = subj_posi+obj_posi-st_posi\n",
    "    mask = tf.gather_nd(mask_mat,tf.concat([st_posi+1,ed_posi-1],axis=1))\n",
    "    ner = ner*mask\n",
    "    tokens = tokens*mask\n",
    "    res = tf.cond(attr=='NER',lambda:ner,lambda:tokens)\n",
    "    return res\n",
    "\n",
    "def get_other_posi(POSI,attr,arg,mask_mat,c):\n",
    "    assert POSI=='Left' or POSI=='Right'\n",
    "    batch_size = c.get_shape()[0]\n",
    "    subj_posi = c[:, -4:-3]\n",
    "    obj_posi = c[:, -3:-2]\n",
    "    seqlen = (c.get_shape()[1] - 4) // 2\n",
    "    tokens = c[:, 0:seqlen]\n",
    "    ner = c[:, seqlen:seqlen * 2]\n",
    "    posi = tf.cond(arg=='ArgY',lambda:obj_posi,lambda:subj_posi)\n",
    "\n",
    "    mask = tf.cond(POSI=='Left',lambda:tf.gather_nd(mask_mat,tf.concat([0*posi,posi-1],axis=1)),\n",
    "                   lambda:tf.gather_nd(mask_mat,tf.concat([posi+1,(0*posi+1)*(seqlen-1)],axis=1)))\n",
    "\n",
    "    new_ner = mask*ner\n",
    "    new_tokens = mask*tokens\n",
    "    res = tf.cond(attr=='NER',lambda:new_ner,lambda:new_tokens)\n",
    "    return res\n",
    "\n",
    "def get_range(attr,arg,range_,mask_mat,c):\n",
    "    subj_posi = c[:, -4:-3]\n",
    "    obj_posi = c[:, -3:-2]\n",
    "    seqlen = (c.get_shape()[1] - 4) // 2\n",
    "    tokens = c[:, 0:seqlen]\n",
    "    ner = c[:, seqlen:seqlen * 2]\n",
    "    posi = tf.cond(arg == 'ArgY', lambda: obj_posi, lambda: subj_posi)\n",
    "    res = tf.cond(attr == 'NER', lambda: ner, lambda: tokens)\n",
    "    mask = tf.gather_nd(mask_mat,tf.concat([tf.math.maximum(0,posi-range_),tf.math.minimum(seqlen-1,posi+range_)],axis=1))\n",
    "    res = res*mask\n",
    "    return res\n",
    "\n",
    "def In(w,seq):\n",
    "    assert w in NER2ID\n",
    "    idx = NER2ID[w]\n",
    "    eq = tf.equal(seq,idx)\n",
    "    sum = tf.reshape(tf.reduce_sum(eq,axis=1),[1,-1])\n",
    "    return tf.cast(sum,tf.bool)\n",
    "\n",
    "\n",
    "def merge_soft(x,y):\n",
    "    if type(x)!= tuple:\n",
    "        x = [x]\n",
    "    if type(y)!= tuple:\n",
    "        y = [y]\n",
    "    x = list(x)\n",
    "    y = list(y)\n",
    "    return tuple(x+y)\n",
    "\n",
    "#function for $Is\n",
    "def IsFunc_soft(ws,ps,label_mat,keyword_dict,mask_mat,c):\n",
    "    if isinstance(ps,tuple):\n",
    "        bool_list  = []\n",
    "        for p in ps:\n",
    "            if isinstance(ws, tuple):\n",
    "                if ws[0] in Selection:\n",
    "                    bool_list.append(p(ws)(label_mat,keyword_dict,mask_mat)(c))\n",
    "                else:\n",
    "                    bool_list.append(tf.maximum(sum([p(w)(label_mat,keyword_dict,mask_mat)(c) for w in ws])-len(ws)+1,0.0))\n",
    "            else:\n",
    "                bool_list.append(p(ws)(label_mat,keyword_dict,mask_mat)(c))\n",
    "        return tf.maximum(sum(bool_list)-len(bool_list)+1,0.0)\n",
    "\n",
    "    if isinstance(ws,tuple):\n",
    "        if ws[0] in Selection:\n",
    "            return ps(ws)(label_mat,keyword_dict,mask_mat)(c)\n",
    "        else:\n",
    "            return tf.maximum(sum([ps(w)(label_mat,keyword_dict,mask_mat)(c) for w in ws])-len(ws)+1,0.0)\n",
    "    else:\n",
    "        return ps(ws)(label_mat,keyword_dict,mask_mat)(c)\n",
    "\n",
    "#function for @Left and @Right\n",
    "def at_POSI_soft(POSI,ws,arg,label_mat,keyword_dict,mask_mat,c,option=None):\n",
    "    if isinstance(ws,tuple):\n",
    "        if ws[0] in Selection:\n",
    "            assert POSI=='Left' or POSI=='Right' or POSI=='Range'\n",
    "            if POSI=='Left' or POSI=='Right':\n",
    "                score_raw = tf.cast(In(ws[1],get_other_posi(POSI,ws[0],arg,mask_mat,c)),tf.float32)\n",
    "                return score_raw\n",
    "            else:\n",
    "                score_raw =  tf.cast(In(ws[1], get_range(ws[0], arg, option['range'],mask_mat,c)), tf.float32)\n",
    "                return score_raw\n",
    "        else:\n",
    "            bool_list = []\n",
    "            for w in ws:\n",
    "                bool_list.append(at_POSI_0_soft(POSI,arg,w,label_mat,keyword_dict,mask_mat,c,option))\n",
    "            return tf.maximum(sum(bool_list)-len(bool_list)+1,0.0)\n",
    "    else:\n",
    "        return at_POSI_0_soft(POSI,arg,ws,label_mat,keyword_dict,mask_mat,c,option)\n",
    "\n",
    "#function for @Left0 and @Right0\n",
    "def at_POSI_0_soft(POSI,arg,w,label_mat,keyword_dict,mask_mat,c,option=None):\n",
    "    if arg not in ['ArgX','ArgY']:\n",
    "        w,arg = arg,w\n",
    "        if POSI == 'Left':\n",
    "            POSI = 'Right'\n",
    "        elif POSI == 'Right':\n",
    "            POSI = 'Left'\n",
    "\n",
    "    if isinstance(w,tuple) and w[0] not in Selection:\n",
    "        return tf.maximum(sum([at_POSI_0_soft(POSI,arg,ww,label_mat,keyword_dict,mask_mat,c,option) for ww in w])-len(w)+1,0.0)\n",
    "\n",
    "\n",
    "    if option==None:\n",
    "        option = {'attr': 'word', 'range': -1, 'numAppear':1,'cmp':'nlt','onlyCount':False}  #For now, if onlyCount==True, then attr=='tokens'\n",
    "    if isinstance(w,tuple):\n",
    "        assert POSI == 'Left' or POSI == 'Right' or POSI == 'Range'\n",
    "        if POSI == 'Left' or POSI == 'Right':\n",
    "            score_raw = tf.cast(In(ws[1], get_other_posi(POSI, ws[0], arg, mask_mat,c)), tf.float32)\n",
    "            return score_raw\n",
    "        else:\n",
    "            score_raw = tf.cast(In(ws[1], get_range(ws[0], arg, option['range'], mask_mat,c)), tf.float32)\n",
    "            return score_raw\n",
    "    else:                                                                                   #For now,option['attr']==tokens if and only if 'right before' is used or onlyCount==True, otherwise ==word\n",
    "        if option == 'Direct':\n",
    "            w_split = w.split()\n",
    "            while '' in w_split:\n",
    "                w_split.remove('')\n",
    "            range_ = len(w_split)\n",
    "\n",
    "            subj_posi = c[:, -4:-3]\n",
    "            obj_posi = c[:, -3:-2]\n",
    "            seqlen = (c.get_shape()[1] - 4) // 2\n",
    "\n",
    "            if arg=='ArgY':\n",
    "                arg_posi = obj_posi\n",
    "            else:\n",
    "                arg_posi = subj_posi\n",
    "\n",
    "            if POSI == 'Left':\n",
    "                st = tf.math.maximum(0, arg_posi - range_)\n",
    "                position = tf.concat([st,arg_posi-1],axis=1)\n",
    "                unmatch_position = tf.concat([tf.math.maximum(0,arg_posi - range_ - unmatch_match_dist), arg_posi - 1], axis=1)\n",
    "            elif POSI == 'Right':\n",
    "                st = arg_posi\n",
    "                ed = tf.math.minimum(seqlen - 1, arg_posi+range_)\n",
    "                position = tf.concat([st+1,ed],axis=1)\n",
    "                unmatch_position = tf.concat([st + 1, tf.math.minimum(seqlen - 1,st+range_+unmatch_match_dist)], axis=1)\n",
    "            else:\n",
    "                raise ValueError\n",
    "            if w in ['ArgX','ArgY']:\n",
    "                if w=='ArgX':\n",
    "                    tar = c[:,-4:-3]\n",
    "                    src = c[:,-3:-2]\n",
    "                else:\n",
    "                    src = c[:,-4:-3]\n",
    "                    tar = c[:,-3:-2]\n",
    "                if POSI=='Left':\n",
    "                    score_raw = tf.reshape(tf.cast(tf.equal(src-tar,1),tf.float32),[1,-1])\n",
    "                    return score_raw\n",
    "                else:\n",
    "                    score_raw = tf.reshape(tf.cast(tf.equal(tar-src, 1),tf.float32),[1,-1])\n",
    "                    return score_raw\n",
    "            else:\n",
    "                mask_raw = tf.cast(tf.gather_nd(mask_mat, position), tf.float32)\n",
    "                mask_raw = mask_raw+(tf.cast(tf.gather_nd(mask_mat, unmatch_position), tf.float32)-mask_raw)*unmatch_match_score\n",
    "                return tf.reshape(tf.reduce_max(label_mat[:, :, keyword_dict[w]] * (mask_raw),axis=1), [1, -1])\n",
    "\n",
    "        if option['range']==-1:\n",
    "            assert POSI=='Left' or POSI=='Right'\n",
    "            batch_size = c.get_shape()[0]\n",
    "            subj_posi = c[:, -4:-3]\n",
    "            obj_posi = c[:, -3:-2]\n",
    "            seqlen = (c.get_shape()[1] - 4) // 2\n",
    "            if arg == 'ArgY':\n",
    "                arg_posi = obj_posi\n",
    "            else:\n",
    "                arg_posi = subj_posi\n",
    "\n",
    "            if POSI=='Left':\n",
    "                position = tf.concat([0*arg_posi, arg_posi - 1], axis=1)\n",
    "                unmatch_position = position\n",
    "            else:\n",
    "                position = tf.concat([arg_posi + 1, (0*arg_posi+1) * (seqlen - 1)],axis=1)\n",
    "                unmatch_position = position\n",
    "\n",
    "        else:                                                                               #For now, if range!=-1 then attr == 'tokens'\n",
    "            subj_posi = c[:, -4:-3]\n",
    "            obj_posi = c[:, -3:-2]\n",
    "            seqlen = (c.get_shape()[1] - 4) // 2\n",
    "            if arg == 'ArgY':\n",
    "                arg_posi = obj_posi\n",
    "            else:\n",
    "                arg_posi = subj_posi\n",
    "\n",
    "            range_ = option['range']\n",
    "\n",
    "            if POSI == 'Left':\n",
    "                st = tf.math.maximum(0, arg_posi - range_)\n",
    "                position = tf.concat([st, arg_posi-1],axis=1)\n",
    "                unmatch_position = tf.concat([tf.math.maximum(0, arg_posi - range_ - unmatch_match_dist), arg_posi - 1],axis=1)\n",
    "            elif POSI=='Right':\n",
    "                st = arg_posi\n",
    "                ed = tf.math.minimum(seqlen - 1, arg_posi+range_)\n",
    "                position = tf.concat([arg_posi+1, ed],axis=1)\n",
    "                unmatch_position = tf.concat([st + 1, tf.math.minimum(seqlen - 1, st + range_ + unmatch_match_dist)],axis=1)\n",
    "            else:\n",
    "                st = tf.math.maximum(0,arg_posi-range_)\n",
    "                ed = tf.math.minimum(seqlen-1,arg_posi+range_)\n",
    "                position=tf.concat([st,ed],axis=1)\n",
    "                unmatch_position = tf.concat([tf.math.maximum(0, arg_posi - range_ - unmatch_match_dist), tf.math.minimum(seqlen - 1, st + range_ + unmatch_match_dist)], axis=1)\n",
    "\n",
    "        if option['onlyCount']:\n",
    "            score_raw = tf.reshape(tf.cast(compare_soft[option['cmp']](position[:,1]-position[:,0],option['numAppear']),tf.float32),[1,-1])\n",
    "            return score_raw\n",
    "        else:\n",
    "            assert option['cmp']=='nlt' and option['numAppear']==1\n",
    "            if w in ['ArgX','ArgY']:\n",
    "                if w=='ArgX':\n",
    "                    tar = c[:,-4]\n",
    "                else:\n",
    "                    tar = c[:,-3]\n",
    "                score_raw = tf.reshape(tf.cast(tf.logical_and(tar>=position[:,0],tar<=position[:,1]),tf.float32),[1,-1])\n",
    "                return score_raw\n",
    "            else:\n",
    "                mask_raw = tf.cast(tf.gather_nd(mask_mat, position),tf.float32)\n",
    "                mask_raw = mask_raw+(tf.cast(tf.gather_nd(mask_mat, unmatch_position),tf.float32)-mask_raw)*unmatch_match_score\n",
    "                return tf.reshape(tf.reduce_max(label_mat[:, :, keyword_dict[w]] * (mask_raw),axis=1), [1, -1])\n",
    "\n",
    "\n",
    "\n",
    "#function for @Between\n",
    "\n",
    "def at_between_soft(w,label_mat,keyword_dict,mask_mat,c,option=None):\n",
    "    if option==None:\n",
    "        option = {'attr': 'word', 'numAppear':1,'cmp':'nlt','onlyCount':False}                #For now, if onlyCount==True, then attr=='tokens'\n",
    "    if isinstance(w,tuple):\n",
    "        score_raw = tf.cast(In(w[1],get_mid(w[0],mask_mat,c)),tf.float32)\n",
    "        return score_raw\n",
    "    else:                                                                                   #For now,option['attr']==tokens if and only if  onlyCount==True, otherwise ==word\n",
    "        if option['onlyCount']:\n",
    "            score_raw =  tf.reshape(tf.cast(compare_soft[option['cmp']](tf.abs(c[:,-4]-c[:,-3])-1,option['numAppear']),tf.float32),[1,-1])\n",
    "            return score_raw\n",
    "        else:\n",
    "            assert option['cmp'] == 'nlt' and option['numAppear'] == 1\n",
    "            l_posi = tf.math.minimum(c[:,-4],c[:,-3])\n",
    "            g_posi = c[:,-4]+c[:,-3]-l_posi\n",
    "            if w in ['ArgX','ArgY']:\n",
    "                if w=='ArgX':\n",
    "                    tar = c[:,-4]\n",
    "                else:\n",
    "                    tar = c[:,-3]\n",
    "                score_raw = tf.reshape(tf.cast(tf.logical_and(tar>l_posi,tar<g_posi),tf.float32),[1,-1])\n",
    "                return score_raw\n",
    "            else:\n",
    "                l_posi = tf.reshape(l_posi,[-1,1])+1\n",
    "                g_posi = tf.reshape(g_posi, [-1, 1])-1\n",
    "                position = tf.concat([l_posi, g_posi],axis=1)\n",
    "                mask_raw = tf.cast(tf.gather_nd(mask_mat, position),tf.float32)\n",
    "                seqlen = (c.get_shape()[1] - 4) // 2\n",
    "                return tf.reshape(tf.reduce_max(label_mat[:, :, keyword_dict[w]] * (mask_raw),axis=1), [1, -1])\n",
    "\n",
    "\n",
    "\n",
    "#function for counting\n",
    "def at_lessthan_soft(funcx,nouny,w,label_mat,keyword_dict,mask_mat,c):\n",
    "    if w=='There':\n",
    "        onlyCount=True\n",
    "    else:\n",
    "        onlyCount = False\n",
    "    if onlyCount:\n",
    "        return funcx(w,{'attr':nouny['attr'],'range':-1,'numAppear':nouny['num'],'cmp':'lt','onlyCount':onlyCount})(label_mat,keyword_dict,mask_mat)(c)                #There are less than 3 words before OBJ\n",
    "    else:\n",
    "        return funcx(w, {'attr': nouny['attr'], 'range': nouny['num'], 'numAppear': 1, 'cmp': 'nlt','onlyCount': onlyCount})(label_mat,keyword_dict,mask_mat)(c)      #the word 'x' is less than 3 words before OBJ\n",
    "\n",
    "def at_atmost_soft(funcx,nouny,w,label_mat,keyword_dict,mask_mat,c):\n",
    "    if w=='There':\n",
    "        onlyCount=True\n",
    "    else:\n",
    "        onlyCount = False\n",
    "    if onlyCount:\n",
    "        return funcx(w,{'attr':nouny['attr'],'range':-1,'numAppear':nouny['num'],'cmp':'nmt','onlyCount':onlyCount})(label_mat,keyword_dict,mask_mat)(c)                #There are at most 3 words before OBJ\n",
    "    else:\n",
    "        return funcx(w, {'attr': nouny['attr'], 'range': nouny['num'], 'numAppear': 1, 'cmp': 'nlt','onlyCount': onlyCount})(label_mat,keyword_dict,mask_mat)(c)      #the word 'x' is at most 3 words before OBJ\n",
    "\n",
    "def at_atleast_soft(funcx,nouny,w,label_mat,keyword_dict,mask_mat,c):\n",
    "    if w=='There':\n",
    "        onlyCount=True\n",
    "    else:\n",
    "        onlyCount = False\n",
    "    if onlyCount:\n",
    "        return funcx(w,{'attr':nouny['attr'],'range':-1,'numAppear':nouny['num'],'cmp':'nlt','onlyCount':onlyCount})(label_mat,keyword_dict,mask_mat)(c)             #There are at least 3 words before OBJ\n",
    "    else:\n",
    "        return funcx(w,{'attr': nouny['attr'], 'range': nouny['num'], 'numAppear': 1, 'cmp': 'nlt','onlyCount': onlyCount})(label_mat,keyword_dict,mask_mat)(c)      #the word 'x' is no less than 3 words before OBJ\n",
    "\n",
    "def at_morethan_soft(funcx,nouny,w,label_mat,keyword_dict,mask_mat,c):\n",
    "    if w=='There':\n",
    "        onlyCount=True\n",
    "    else:\n",
    "        onlyCount = False\n",
    "    if onlyCount:\n",
    "        return funcx(w,{'attr':nouny['attr'],'range':-1,'numAppear':nouny['num'],'cmp':'mt','onlyCount':onlyCount})(label_mat,keyword_dict,mask_mat)(c)                #There are more than 3 words before OBJ\n",
    "    else:\n",
    "        return funcx(w, {'attr': nouny['attr'], 'range': nouny['num'], 'numAppear': 1, 'cmp': 'nlt','onlyCount': onlyCount})(label_mat,keyword_dict,mask_mat)(c)    #the word 'x' is more than 3 words before OBJ\n",
    "\n",
    "\n",
    "#function for @In0\n",
    "\n",
    "def at_In0_soft(arg,w,label_mat,keyword_dict,mask_mat,c):\n",
    "    assert arg=='Sentence'                                                                                                          #Right?\n",
    "    if isinstance(w,tuple):\n",
    "        seqlen = (c.get_shape()[1] - 4) // 2\n",
    "        score_raw = tf.cast(In(w,c[:,seqlen:seqlen*2]),tf.float32)\n",
    "        return score_raw\n",
    "    else:\n",
    "        seqlen = (c.get_shape()[1] - 4) // 2\n",
    "        return tf.reshape(tf.reduce_max(label_mat[:, :, keyword_dict[w]], axis=1), [1, -1])\n",
    "        # return w in c.sentence\n",
    "\n",
    "def at_WordCount_soft(nounNum,nouny,F,label_mat,keyword_dict,mask_mat,c):\n",
    "    if isinstance(nouny,tuple):\n",
    "        return tf.maximum(tf.maximum(sum([F(noun, option={'attr': 'word', 'range': -1, 'numAppear': 1, 'cmp': 'nlt', 'onlyCount': False})(label_mat,keyword_dict,mask_mat)(c) for noun in nouny])-len(nouny)+1,0.0)+F(nouny[0],option={'attr': 'tokens','range': -1,'numAppear':sum([len(noun.split()) for noun in nouny]),'cmp': 'eq','onlyCount': True})(label_mat,keyword_dict,mask_mat)(c)-1,0.0)\n",
    "    else:\n",
    "        return tf.maximum(F(nouny, option={'attr':'word','range':-1,'numAppear':1,'cmp':'nlt','onlyCount':False})(label_mat,keyword_dict,mask_mat)(c)+F(nouny, option={'attr':'tokens','range':-1,'numAppear':len(nouny.split()),'cmp':'eq','onlyCount':True})(label_mat,keyword_dict,mask_mat)(c)-1,0.0)\n",
    "\n",
    "ops_soft={\n",
    "    \".root\": lambda xs: lambda label_mat,keyword_dict,mask_mat:lambda c: tf.maximum(sum([x(label_mat,keyword_dict,mask_mat)(c) for x in xs])-len(xs)+1,0.0) if type(xs) == tuple else xs(label_mat,keyword_dict,mask_mat)(c),\n",
    "    \"@Word\": lambda x: x,\n",
    "    \"@Is\": lambda ws, p: lambda label_mat,keyword_dict,mask_mat:lambda c: IsFunc_soft(ws, p,label_mat,keyword_dict,mask_mat, c),\n",
    "    \"@between\": lambda a: lambda w, option=None: lambda label_mat,keyword_dict,mask_mat:lambda c: at_between_soft(w, label_mat,keyword_dict,mask_mat,c, option),\n",
    "    \"@And\": lambda x, y: merge_soft(x, y),\n",
    "    \"@Num\": lambda x, y: {'attr': y, \"num\": int(x)},\n",
    "    \"@LessThan\": lambda funcx, nouny: lambda w: lambda label_mat,keyword_dict,mask_mat:lambda c: at_lessthan_soft(funcx, nouny, w, label_mat,keyword_dict,mask_mat,c),\n",
    "    \"@AtMost\": lambda funcx, nouny: lambda w: lambda label_mat,keyword_dict,mask_mat:lambda c: at_atmost_soft(funcx, nouny, w, label_mat,keyword_dict,mask_mat,c),\n",
    "    \"@AtLeast\": lambda funcx, nouny: lambda w: lambda label_mat,keyword_dict,mask_mat:lambda c: at_atleast_soft(funcx, nouny, w, label_mat,keyword_dict,mask_mat,c),\n",
    "    \"@MoreThan\": lambda funcx, nouny: lambda w: lambda label_mat,keyword_dict,mask_mat:lambda c: at_morethan_soft(funcx, nouny, w, label_mat,keyword_dict,mask_mat,c),\n",
    "    \"@WordCount\": lambda nounNum, nouny, F:lambda useless: lambda label_mat,keyword_dict,mask_mat:lambda c: at_WordCount_soft(nounNum,nouny,F,label_mat,keyword_dict,mask_mat,c),\n",
    "\n",
    "    \"@NumberOf\": lambda x, f: [x, f],\n",
    "    \"@LessThan1\": lambda nounynum: lambda x: lambda label_mat,keyword_dict,mask_mat:lambda c: at_lessthan_soft(x[1], {'attr': x[0], \"num\": int(nounynum)}, 'There',label_mat,keyword_dict,mask_mat,c),\n",
    "    \"@AtMost1\": lambda nounynum: lambda x: lambda label_mat,keyword_dict,mask_mat:lambda c: at_atmost_soft(x[1], {'attr': x[0], \"num\": int(nounynum)}, 'There', label_mat,keyword_dict,mask_mat,c),\n",
    "    \"@AtLeast1\": lambda nounynum: lambda x: lambda label_mat,keyword_dict,mask_mat:lambda c: at_atleast_soft(x[1], {'attr': x[0], \"num\": int(nounynum)}, 'There',label_mat,keyword_dict,mask_mat,c),\n",
    "    \"@MoreThan1\": lambda nounynum: lambda x: lambda label_mat,keyword_dict,mask_mat:lambda c: at_morethan_soft(x[1], {'attr': x[0], \"num\": int(nounynum)}, 'There',label_mat,keyword_dict,mask_mat,c),\n",
    "\n",
    "    \"@In0\": lambda arg: lambda w: lambda label_mat,keyword_dict,mask_mat:lambda c: at_In0_soft(arg, w, label_mat,keyword_dict,mask_mat,c),\n",
    "\n",
    "    \"@By\": lambda x, f, z: lambda label_mat,keyword_dict,mask_mat:lambda c: f(x, {'attr': z['attr'], 'range': z['num'], 'numAppear': 1, 'cmp': 'nlt','onlyCount': False})(label_mat,keyword_dict,mask_mat)(c),\n",
    "\n",
    "    \"@Left0\": lambda arg: lambda w, option=None: lambda label_mat,keyword_dict,mask_mat:lambda c: at_POSI_0_soft('Left', arg, w, label_mat,keyword_dict,mask_mat,c, option),\n",
    "\n",
    "    \"@Right0\": lambda arg: lambda w, option=None: lambda label_mat,keyword_dict,mask_mat:lambda c: at_POSI_0_soft('Right', arg, w, label_mat,keyword_dict,mask_mat,c, option),\n",
    "\n",
    "    \"@Range0\": lambda arg: lambda w, option=None: lambda label_mat,keyword_dict,mask_mat:lambda c: at_POSI_0_soft('Range', arg, w, label_mat,keyword_dict,mask_mat,c, option),\n",
    "\n",
    "    \"@Left\": lambda arg, ws, option=None: lambda label_mat,keyword_dict,mask_mat:lambda c: at_POSI_soft('Left', ws, arg, label_mat,keyword_dict,mask_mat,c, option),\n",
    "    \"@Right\": lambda arg, ws, option=None: lambda label_mat,keyword_dict,mask_mat:lambda c: at_POSI_soft('Right', ws, arg, label_mat,keyword_dict,mask_mat,c, option),\n",
    "\n",
    "    \"@Direct\": lambda func: lambda w: lambda label_mat,keyword_dict,mask_mat:lambda c: func(w, 'Direct')(label_mat,keyword_dict,mask_mat)(c)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini_sent = \"\"\"\n",
    "# The phrase \\\"'s grandmother\\\" occurs between SUBJ and OBJ\n",
    "# \"\"\"\n",
    "\n",
    "longer_sent = \"\"\"\n",
    "The phrase \\\"'s grandmother\\\" occurs between SUBJ and OBJ and there are no more than four words between SUBJ and OBJ\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = longer_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process = CCG.utils.pre_process_sent(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'phrase',\n",
       " '\"\\'s grandmother\"',\n",
       " 'occurs',\n",
       " 'between',\n",
       " 'SUBJ',\n",
       " 'and',\n",
       " 'OBJ',\n",
       " 'and',\n",
       " 'there',\n",
       " 'are',\n",
       " '$AtMost',\n",
       " 'four',\n",
       " 'words',\n",
       " 'between',\n",
       " 'SUBJ',\n",
       " 'and',\n",
       " 'OBJ']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenized = CCG.utils.print_tokenized(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['$The',\n",
       "  '$Word',\n",
       "  '\"\\'s grandmother\"',\n",
       "  '$Is',\n",
       "  '$Between',\n",
       "  '$ArgX',\n",
       "  '$And',\n",
       "  '$ArgY',\n",
       "  '$And',\n",
       "  '$There',\n",
       "  '$Is',\n",
       "  '$AtMost',\n",
       "  \"'4'\",\n",
       "  '$Word',\n",
       "  '$Between',\n",
       "  '$ArgX',\n",
       "  '$And',\n",
       "  '$ArgY']]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.ccg import chart, lexicon\n",
    "# from stanfordcorenlp import StanfordCoreNLP\n",
    "import CCG.constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$The',\n",
       " '$Word',\n",
       " '\"\\'s grandmother\"',\n",
       " '$Is',\n",
       " '$Between',\n",
       " '$ArgX',\n",
       " '$And',\n",
       " '$ArgY',\n",
       " '$And',\n",
       " '$There',\n",
       " '$Is',\n",
       " '$AtMost',\n",
       " \"'4'\",\n",
       " '$Word',\n",
       " '$Between',\n",
       " '$ArgX',\n",
       " '$And',\n",
       " '$ArgY']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"''s grandmother'\", \"'4'\"]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CCG.utils.new_predicate(sent_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_lexicon = CCG.constant.raw_lexicon\n",
    "predicates = CCG.utils.new_predicate(sent_tokenized)\n",
    "raw_lexicon = CCG.utils.add_new_predicate(predicates, raw_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\":- S,NP,N,PP\\n        VP :: S\\\\NP\\n        Det :: NP/N\\n        Adj :: N/N\\n        arg => NP {None}\\n        #$True => (S\\\\VP)/PP {None}\\n        #$False => (S\\\\VP)/PP {None}\\n        $And => var\\\\.,var/.,var {\\\\x y.'@And'(x,y)}\\n        $Or => var\\\\.,var/.,var {\\\\x y.'@Or'(x,y)}\\n        $Not => (S\\\\NP)\\\\(S\\\\NP) {None}\\n        $Not => (S\\\\NP)/(S\\\\NP) {None}\\n        $All => NP/N {None}\\n        $All => NP {None}\\n        $All => NP/NP {None}\\n        $Any => NP/N {None}\\n        $None => N {None}\\n        #$Is => (S\\\\NP)/NP {\\\\y x.'@Is'(x,y)}\\n        #$Is => (S\\\\NP)/(S\\\\NP) {\\\\y x.'@Is'(x,y)}\\n        $Is => (S\\\\NP)/PP {\\\\y x.'@Is'(x,y)}   # word 'a' occurs between <S> and <O>\\n        $Is => (S\\\\NP)\\\\PP {\\\\y x.'@Is'(x,y)}  # between <S> and <O> occurs word 'a'\\n        $Is => (S\\\\PP)\\\\NP {\\\\x y.'@Is'(x,y)}  # between <S> and <O> word 'a' occurs\\n        $Exists => S\\\\NP/PP {\\\\y x.'@Is'(x,y)}\\n        #$Exists => S\\\\NP {None}\\n        $Int => Adj {None} #There are no words between <S> and <O>\\n        $AtLeastOne => NP/N {None}\\n        #$Equals => (S\\\\NP)/NP {None}\\n        #$NotEquals => (S\\\\NP)/NP {None}\\n        \\n        \\n        $LessThan => PP/PP/N {\\\\x y.'@LessThan'(y,x)} #There are less than 3 words between <S> and <O>   \\n        $AtMost => PP/PP/N {\\\\x y.'@AtMost'(y,x)} #There are at most 3 words between <S> and <O>\\n        $AtLeast => PP/PP/N {\\\\x y.'@AtLeast'(y,x)} #same as above\\n        $MoreThan => PP/PP/N {\\\\x y.'@MoreThan'(y,x)} #same as above\\n        \\n        $LessThan => PP/N {\\\\x.'@LessThan1'(y,x)} #number of words between X and Y is less than 7.\\n        $AtMost => PP/N {\\\\x.'@AtMost1'(y,x)} \\n        $AtLeast => PP/N {\\\\x.'@AtLeast1'(y,x)}   #same as above\\n        $MoreThan => PP/N {\\\\x.'@MoreThan1'(y,x)} #same as above\\n\\n        #$In => S\\\\NP/NP {None} \\n        $In => PP/NP {\\\\x.'@In0'(x)} \\n        $Contains => S\\\\NP/NP {None} #The sentence contains two words\\n        $Separator => var\\\\.,var/.,var {\\\\x y.'@And'(x,y)} #connection between two words\\n        #$Processive => NP/N\\\\N {None}\\n        #$Count => N {None}\\n        #$Tuple => N {None}\\n        #$ArgXListAnd => NP {None}\\n        $EachOther => N {None}\\n        $Token => N {\\\\x.'@Word'(x)}\\n        $Word => NP/N {\\\\x.'@Word'(x)}\\n        $Word => NP/NP {\\\\x.'@Word'(x)}\\n        \\n        $Word => N {'tokens'} #There are no more than 3 words between <S> and <O>\\n        $Word => NP {'tokens'} #There are no more than 3 words between <S> and <O>\\n        \\n        $Char => N {None} #same as above\\n        #$Lower => Adj {None}\\n        #$Capital => Adj {None}\\n        $StartsWith => S\\\\NP/NP {\\\\y x.'@StartsWith'(x,y)}\\n        $EndsWith => S\\\\NP/NP {\\\\y x.'@EndsWith'(x,y)}\\n        $Left => PP/NP {\\\\x.'@Left0'(x)} # the word 'a' is before <S>\\n        $Left => (S\\\\NP)/NP {\\\\y x.'@Left'(y,x)}  #Precedes\\n        $Right => PP/NP {\\\\x.'@Right0'(x)}# the word 'a' ia after <S>\\n        $Right => (S\\\\NP)/NP {\\\\y x.'@Right'(y,x)} \\n        #$Within => ((S\\\\NP)\\\\(S\\\\NP))/NP {None} # the word 'a' is within 2 words after <S>\\n        #$Within => (NP\\\\NP)/NP {None}\\n        $Within => PP/PP/N {\\\\x y.'@AtMost'(y,x)} #Does Within has other meaning.\\n        $Sentence => NP {'Sentence'}\\n        \\n        $Between => (S/S)/NP {\\\\x y.'@between'(x,y)}\\n        $Between => S/NP {\\\\x.'@between'(x)}\\n        $Between => PP/NP {\\\\x.'@between'(x)}\\n        $Between => (NP\\\\NP)/NP {\\\\x y.'@between'(x,y)}\\n        \\n        $PersonNER => NP {'@PER'}\\n        $LocationNER => NP {'@LOC'}\\n        $DateNER => NP {'@Date'}\\n        $NumberNER => NP {'@Num'}\\n        $OrganizationNER => NP {'@Org'}\\n        $NorpNER => NP {'@Norp'}\\n        $ArgX => NP {'ArgX'}\\n        $ArgY => NP {'ArgY'}\\n        #$will => S\\\\NP/VP {None}\\n        #$Which => (NP\\\\NP)/(S/NP) {None}\\n        #$might => S\\\\NP/VP {None}\\n        $that => NP/N {None}\\n        #$that => (N\\\\N)/(S/NP) {None} #same as which\\n        $Apart => (S/PP)\\\\NP {None}\\n        $Direct => PP/PP {\\\\x.'@Direct'(x)} # the word 'a' is right before <S>   \\n        $Direct => (S\\\\NP)/PP {\\\\y x.'@Is'(x,'@Direct'(y))}\\n        $Last => Adj {None}\\n        $There => NP {'There'}\\n        $By => S\\\\NP\\\\PP/NP {\\\\z f x.'@By'(x,f,z)} #precedes sth by 10 chatacters       \\n        $By => (S\\\\NP)\\\\PP/(PP/PP) {\\\\F x y.'@Is'(y,F(x))} #precedes sth by no more than10 chatacters        \\n        $By => PP\\\\PP/(PP/PP) {\\\\F x. F(x)} #occurs before by no...\\n        \\n        $Numberof => NP/PP/NP {\\\\x F.'@NumberOf'(x,F)}\\n        \\n        $Of => PP/NP {\\\\x.'@Range0'(x)} # the word 'x' is at most 3 words of Y     \\n        $Of => NP/NP {\\\\x.x} #these two are designed to solve problems like $Is $Left $Of and $Is $Left\\n        $Of => N/N {\\\\x.x}\\n        $Char => NP/N {None}\\n        $ArgX => N {'ArgX'}\\n        $ArgY => N {'ArgY'}\\n        $Link => (S\\\\NP)/NP {\\\\x y.'@Is'(y,'@between'(x))}\\n        $SandWich => (S\\\\NP)/NP {\\\\x y.'@Is'(x,'@between'(y))}\\n        $The => N/N {\\\\x.x}\\n        $The =>NP/NP {\\\\x.x}\\n        \\n\\t\\t''sSPACEgrandmother' => NP {''sSPACEgrandmother'}\\n\\t\\t''sSPACEgrandmother' => N {''sSPACEgrandmother'}\\n\\t\\t'4' => NP/NP {\\\\x.'@Num'('4',x)}\\n\\t\\t'4' => N/N {\\\\x.'@Num'('4',x)}\\n\\t\\t'4' => NP {'4'}\\n\\t\\t'4' => N {'4'}\\n\\t\\t'4' => PP/PP/NP/NP {\\\\x y F.'@WordCount'('@Num'('4',x),y,F)}\\n\\t\\t'4' => PP/PP/N/N {\\\\x y F.'@WordCount'('@Num'('4',x),y,F)}\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = lexicon.fromstring(raw_lexicon, True)\n",
    "parser = chart.CCGChartParser(lex, chart.DefaultRuleSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_1 in range(len(sent_tokenized)):\n",
    "    while [] in sent_tokenized[index_1]:\n",
    "        sent_tokenized[index_1].remove([])\n",
    "    for index_2 in range(len(sent_tokenized[index_1])):\n",
    "        if sent_tokenized[index_1][index_2].startswith(\"'\") or sent_tokenized[index_1][index_2].startswith(\"\\\"\"):\n",
    "            sent_tokenized[index_1][index_2] = sent_tokenized[index_1][index_2].replace(' ','SPACE')\n",
    "            sent_tokenized[index_1][index_2] = sent_tokenized[index_1][index_2].replace(\"\\\"\",\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['$The',\n",
       "  '$Word',\n",
       "  \"''sSPACEgrandmother'\",\n",
       "  '$Is',\n",
       "  '$Between',\n",
       "  '$ArgX',\n",
       "  '$And',\n",
       "  '$ArgY',\n",
       "  '$And',\n",
       "  '$There',\n",
       "  '$Is',\n",
       "  '$AtMost',\n",
       "  \"'4'\",\n",
       "  '$Word',\n",
       "  '$Between',\n",
       "  '$ArgX',\n",
       "  '$And',\n",
       "  '$ArgY']]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_forms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i, e in enumerate(sent_tokenized):\n",
    "    print(i)\n",
    "    parses = list(parser.parse(e))\n",
    "    logic_forms += parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logic_forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantics = []\n",
    "for form in logic_forms:\n",
    "    semantics.append(str(form.label()[0].semantics()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_semantics = list(set(semantics))\n",
    "len(set_semantics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'@And'('@Is'('There','@AtMost'('@between'('@And'('ArgY','ArgX')),'@Num'('4','tokens'))),'@Is'('@Word'(''sSPACEgrandmother'),'@between'('@And'('ArgY','ArgX'))))\""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_semantics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'@And'('@Is'('@And'('There','ArgY'),'@AtMost'('@between'('@And'('ArgY','ArgX')),'@Num'('4','tokens'))),'@Is'('@Word'(''sSPACEgrandmother'),'@between'('ArgX')))\""
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_semantics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(logic_forms[0].label()[0]._categ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(recurse_soft(set_semantics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = blah.start()\n",
    "# print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantics = []\n",
    "for form in logic_forms:\n",
    "    semantics.append(str(form.label()[0].semantics()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'@And'('@Is'('@And'('There','ArgY'),'@AtMost'('@between'('@And'('ArgY','ArgX')),'@Num'('4','tokens'))),'@Is'('@Word'(''sSPACEgrandmother'),'@between'('ArgX')))\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_tokens = str(logic_forms[0]).split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_tokens_1 = str(logic_forms[1]).split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<nltk.ccg.lexicon.Token object at 0x133d2aba8>, '<')\n",
      "  ((<nltk.ccg.lexicon.Token object at 0x134e07a20>, '>')\n",
      "    ((<nltk.ccg.lexicon.Token object at 0x132f28320>, '<')\n",
      "      ((<nltk.ccg.lexicon.Token object at 0x132f28cc0>, '>')\n",
      "        ((<nltk.ccg.lexicon.Token object at 0x132f28b70>, '>')\n",
      "          ((<nltk.ccg.lexicon.Token object at 0x132f28c88>, '>')\n",
      "            ((<nltk.ccg.lexicon.Token object at 0x13455ba20>, 'Leaf')\n",
      "              (<nltk.ccg.lexicon.Token object at 0x13455ba20>\n",
      "                $AtMost))\n",
      "            ((<nltk.ccg.lexicon.Token object at 0x132f28e80>, '>')\n",
      "                (<nltk.ccg.lexicon.Token object at 0x133b9ec18> '4'))\n",
      "              ((<nltk.ccg.lexicon.Token object at 0x133a8b438>, 'Leaf')\n",
      "                (<nltk.ccg.lexicon.Token object at 0x133a8b438>\n",
      "                  $Word))))\n"
     ]
    }
   ],
   "source": [
    "for token in temp_tokens:\n",
    "    if token not in temp_tokens_1:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"((<nltk.ccg.lexicon.Token object at 0x1341f0550>, '<')\",\n",
       " \"  ((<nltk.ccg.lexicon.Token object at 0x132f1d908>, '<')\",\n",
       " \"    ((<nltk.ccg.lexicon.Token object at 0x132f1d780>, '>')\",\n",
       " \"      ((<nltk.ccg.lexicon.Token object at 0x131a90eb8>, 'Leaf')\",\n",
       " '        (<nltk.ccg.lexicon.Token object at 0x131a90eb8> $The))',\n",
       " \"      ((<nltk.ccg.lexicon.Token object at 0x132f1d978>, '>')\",\n",
       " \"        ((<nltk.ccg.lexicon.Token object at 0x133cdcb70>, 'Leaf')\",\n",
       " '          (<nltk.ccg.lexicon.Token object at 0x133cdcb70> $Word))',\n",
       " \"        ((<nltk.ccg.lexicon.Token object at 0x131a902b0>, 'Leaf')\",\n",
       " '          (<nltk.ccg.lexicon.Token object at 0x131a902b0>',\n",
       " \"            ''sSPACEgrandmother'))))\",\n",
       " \"    ((<nltk.ccg.lexicon.Token object at 0x132f1dd68>, '>')\",\n",
       " \"      ((<nltk.ccg.lexicon.Token object at 0x1341a6128>, 'Leaf')\",\n",
       " '        (<nltk.ccg.lexicon.Token object at 0x1341a6128> $Is))',\n",
       " \"      ((<nltk.ccg.lexicon.Token object at 0x132f1dda0>, '>')\",\n",
       " \"        ((<nltk.ccg.lexicon.Token object at 0x131a681d0>, 'Leaf')\",\n",
       " '          (<nltk.ccg.lexicon.Token object at 0x131a681d0> $Between))',\n",
       " \"        ((<nltk.ccg.lexicon.Token object at 0x133d26080>, 'Leaf')\",\n",
       " '          (<nltk.ccg.lexicon.Token object at 0x133d26080> $ArgX)))))',\n",
       " \"  ((<nltk.ccg.lexicon.Token object at 0x132f285f8>, '>')\",\n",
       " \"    ((<nltk.ccg.lexicon.Token object at 0x133b91710>, 'Leaf')\",\n",
       " '      (<nltk.ccg.lexicon.Token object at 0x133b91710> $And))',\n",
       " \"    ((<nltk.ccg.lexicon.Token object at 0x132f284e0>, '<')\",\n",
       " \"      ((<nltk.ccg.lexicon.Token object at 0x132f28710>, '<')\",\n",
       " \"        ((<nltk.ccg.lexicon.Token object at 0x131a58a20>, 'Leaf')\",\n",
       " '          (<nltk.ccg.lexicon.Token object at 0x131a58a20> $ArgY))',\n",
       " \"        ((<nltk.ccg.lexicon.Token object at 0x132f282b0>, '>')\",\n",
       " \"          ((<nltk.ccg.lexicon.Token object at 0x133b91710>, 'Leaf')\",\n",
       " '            (<nltk.ccg.lexicon.Token object at 0x133b91710> $And))',\n",
       " \"          ((<nltk.ccg.lexicon.Token object at 0x131a27438>, 'Leaf')\",\n",
       " '            (<nltk.ccg.lexicon.Token object at 0x131a27438> $There))))',\n",
       " \"      ((<nltk.ccg.lexicon.Token object at 0x132f28cf8>, '>')\",\n",
       " \"        ((<nltk.ccg.lexicon.Token object at 0x1341a6128>, 'Leaf')\",\n",
       " '          (<nltk.ccg.lexicon.Token object at 0x1341a6128> $Is))',\n",
       " \"        ((<nltk.ccg.lexicon.Token object at 0x132f28f98>, '>')\",\n",
       " \"          ((<nltk.ccg.lexicon.Token object at 0x132f28f60>, '>')\",\n",
       " \"            ((<nltk.ccg.lexicon.Token object at 0x132f28d30>, '>B')\",\n",
       " \"              ((<nltk.ccg.lexicon.Token object at 0x13455ba20>, 'Leaf')\",\n",
       " '                (<nltk.ccg.lexicon.Token object at 0x13455ba20>',\n",
       " '                  $AtMost))',\n",
       " \"              ((<nltk.ccg.lexicon.Token object at 0x133b9ec18>, 'Leaf')\",\n",
       " \"                (<nltk.ccg.lexicon.Token object at 0x133b9ec18> '4')))\",\n",
       " \"            ((<nltk.ccg.lexicon.Token object at 0x133a8b438>, 'Leaf')\",\n",
       " '              (<nltk.ccg.lexicon.Token object at 0x133a8b438> $Word)))',\n",
       " \"          ((<nltk.ccg.lexicon.Token object at 0x132f28ef0>, '>')\",\n",
       " \"            ((<nltk.ccg.lexicon.Token object at 0x131a681d0>, 'Leaf')\",\n",
       " '              (<nltk.ccg.lexicon.Token object at 0x131a681d0>',\n",
       " '                $Between))',\n",
       " \"            ((<nltk.ccg.lexicon.Token object at 0x132f2b2b0>, '<')\",\n",
       " \"              ((<nltk.ccg.lexicon.Token object at 0x133d26080>, 'Leaf')\",\n",
       " '                (<nltk.ccg.lexicon.Token object at 0x133d26080>',\n",
       " '                  $ArgX))',\n",
       " \"              ((<nltk.ccg.lexicon.Token object at 0x132f2b0b8>, '>')\",\n",
       " \"                ((<nltk.ccg.lexicon.Token object at 0x133b91710>, 'Leaf')\",\n",
       " '                  (<nltk.ccg.lexicon.Token object at 0x133b91710>',\n",
       " '                    $And))',\n",
       " \"                ((<nltk.ccg.lexicon.Token object at 0x131a58a20>, 'Leaf')\",\n",
       " '                  (<nltk.ccg.lexicon.Token object at 0x131a58a20>',\n",
       " '                    $ArgY))))))))))']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_tokens_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-ebcc84367179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintCCGDerivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogic_forms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/usc/research/ink_lab/NExT/clouds/lib/python3.6/site-packages/nltk/ccg/chart.py\u001b[0m in \u001b[0;36mprintCCGDerivation\u001b[0;34m(tree)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprintCCGDerivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;31m# Get the leaves and initial categories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m     \u001b[0mleafcats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m     \u001b[0mleafstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0mcatstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'pos'"
     ]
    }
   ],
   "source": [
    "chart.printCCGDerivation(logic_forms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n",
      "<class 'nltk.tree.Tree'>\n"
     ]
    }
   ],
   "source": [
    "for i, obj in enumerate(logic_forms):\n",
    "    print(type(obj))\n",
    "    if i > 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.draw import TreeView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "TreeView(logic_forms[0])._cframe.print_to_file('output.ps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_1 = logic_forms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S {'@And'('@Is'('@And'('There','ArgY'),'@AtMost'('@between'('@And'('ArgY','ArgX')),'@Num'('4','tokens'))),'@Is'('@Word'(''sSPACEgrandmother'),'@between'('ArgX')))}\n"
     ]
    }
   ],
   "source": [
    "strtree_1.label()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S {'@And'('@Is'('@And'('There','ArgY'),'@AtMost'('@between'('@And'('ArgY','ArgX')),'@Num'('4','tokens'))),'@Is'('@Word'(''sSPACEgrandmother'),'@between'('ArgX')))}\n"
     ]
    }
   ],
   "source": [
    "print(tree_2.label()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.ccg.lexicon.Token at 0x13710afd0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_2.label()[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "initial_tree = str(logic_forms[0].label()[0].semantics())\n",
    "for i, tree in enumerate(logic_forms):\n",
    "    str_tree = str(tree.label()[0].semantics())\n",
    "    if str_tree == initial_tree:\n",
    "        print(\"yes\")\n",
    "    else:\n",
    "        print(\"no\")\n",
    "    \n",
    "    if i > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ApplicationExpression '@Is'('@Word'('daughter'),'@between'('@And'('ArgY','ArgX')))>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.label()[0].semantics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.sem.logic.ApplicationExpression"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tree.label()[0].semantics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'@Is'('@Word'('daughter'),'@between'('@And'('ArgY','ArgX')))\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(tree.label()[0].semantics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sem(sem):\n",
    "    sem_split = re.split(',|(\\()',sem)\n",
    "    while None in sem_split:\n",
    "        sem_split.remove(None)\n",
    "    new_sem = []\n",
    "    for i,w in enumerate(sem_split):\n",
    "        if w=='(':\n",
    "            new_sem.insert(-1,'(')\n",
    "        else:\n",
    "            new_sem.append(sem_split[i])\n",
    "    sem_ = ''\n",
    "    for i,s in enumerate(new_sem):\n",
    "        sem_replace = s.replace('SPACE', ' ')\n",
    "        for com in comma_index:\n",
    "            sem_replace = sem_replace.replace('COMMA' + str(comma_index[com]), com)\n",
    "        if sem_replace.startswith(\"'\"):\n",
    "            if sem_replace.endswith(\")\"):\n",
    "                posi = len(sem_replace)-1\n",
    "                while sem_replace[posi]==\")\":\n",
    "                    posi-=1\n",
    "                assert sem_replace[posi]==\"\\'\"\n",
    "            else:\n",
    "                posi = len(sem_replace)-1\n",
    "                assert sem_replace[posi] == \"\\'\"\n",
    "\n",
    "            sem_replace = sem_replace[0]+sem_replace[1:posi].replace('\\'','\\\\\\'')+sem_replace[posi:]\n",
    "        if new_sem[i-1]=='(':\n",
    "            sem_=sem_+sem_replace\n",
    "        else:\n",
    "            sem_ = sem_+','+sem_replace\n",
    "\n",
    "    try:\n",
    "        em = ('.root',eval(sem_[1:]))\n",
    "        return em\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "comma_index = {',':0,'-LRB-':1,'-RRB-':2,'.':3,'-':4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parse_sem(str(tree.label()[0].semantics())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: lambda y: x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>.<locals>.<lambda>(y)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(f(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(1)(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = parse_sem(str(tree.label()[0].semantics()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.root',\n",
       " ('@Is', ('@Word', 'daughter'), ('@between', ('@And', 'ArgY', 'ArgX'))))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(xs)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops[temp[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(temp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurse_soft(sem):\n",
    "    try:\n",
    "        if isinstance(sem, tuple):\n",
    "            op = ops_soft[sem[0]]\n",
    "            args = [recurse_soft(arg) for arg in sem[1:]]\n",
    "            if False in args:\n",
    "                return False\n",
    "            return op(*args) if args else op\n",
    "        else:\n",
    "            if sem in syntax:\n",
    "                return syntax[sem]\n",
    "            else:\n",
    "                return sem\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recurse(temp)(\"SUBJ's daughter appeared between OBJ .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-c59ebb89426e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "np.exp(1)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "hehe = np.zeros([2,1],np.float32)\n",
    "hehe_1 = np.zeros([2,1],np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.dot(hehe.transpose(), hehe_1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AccessDenied",
     "evalue": "psutil.AccessDenied (pid=8634)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/usc/research/ink_lab/NExT/clouds/lib/python3.6/site-packages/psutil/_psosx.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mProcessLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/usc/research/ink_lab/NExT/clouds/lib/python3.6/site-packages/psutil/_psosx.py\u001b[0m in \u001b[0;36mconnections\u001b[0;34m(self, kind)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcatch_zombie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mrawlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamilies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 1] Operation not permitted (originated from proc_pidinfo())",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAccessDenied\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7a65d780233a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordCoreNLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stanford-corenlp-4.1.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/usc/research/ink_lab/NExT/clouds/lib/python3.6/site-packages/stanfordcorenlp/corenlp.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_host, port, memory, lang, timeout, quiet, logging_level)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mport_candidate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m65535\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mport_candidate\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mladdr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpsutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mport_candidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/usc/research/ink_lab/NExT/clouds/lib/python3.6/site-packages/psutil/__init__.py\u001b[0m in \u001b[0;36mnet_connections\u001b[0;34m(kind)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0mOn\u001b[0m \u001b[0mmacOS\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mrequires\u001b[0m \u001b[0mroot\u001b[0m \u001b[0mprivileges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m     \"\"\"\n\u001b[0;32m-> 2157\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_psplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/usc/research/ink_lab/NExT/clouds/lib/python3.6/site-packages/psutil/_psosx.py\u001b[0m in \u001b[0;36mnet_connections\u001b[0;34m(kind)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0mcons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mNoSuchProcess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/usc/research/ink_lab/NExT/clouds/lib/python3.6/site-packages/psutil/_psosx.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPermissionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAccessDenied\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mcext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZombieProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mZombieProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ppid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAccessDenied\u001b[0m: psutil.AccessDenied (pid=8634)"
     ]
    }
   ],
   "source": [
    "nlp = StanfordCoreNLP(\"stanford-corenlp-4.1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
